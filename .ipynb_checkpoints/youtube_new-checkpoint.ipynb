{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f493dc2",
   "metadata": {},
   "source": [
    "# Corpus YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30461b31",
   "metadata": {},
   "source": [
    "## 1. Lectura de datos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc98573f-6644-4c5a-9a08-66dbbe12e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## abrimos los archivos\n",
    "\n",
    "import glob\n",
    "\n",
    "lista_files = glob.glob('YTessays/*.txt')\n",
    "corpus = []\n",
    "\n",
    "for file in lista_files:\n",
    "    with open(file, 'r', encoding='utf8') as f:\n",
    "        corpus+=[f.read()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15522d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## número de textos\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04e0950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"Faceshopping\" by Sophie)\\n♪ My face is the front of shop ♪\\n♪ My face is the real shop front ♪\\n♪ My shop is the face I front ♪\\n♪ I\\'m real when I shop my face ♪\\n♪ Artificial bloom ♪\\n♪ Hydroponic skin ♪\\n♪ Chemical release ♪\\n♪ Synthesize the real ♪\\n(upb'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea81bbb9",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9dd9bf-b5ef-4505-8c67-ad3b187aea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## librerias\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2fce56-63cb-495b-aaf8-fa7feae6b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pauba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20361ab2-6bda-46f4-b9bb-561d725c68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## función para remover () y []\n",
    "\n",
    "def a(test_str):\n",
    "    ret = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in test_str:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == '(':\n",
    "            skip2c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif i == ')'and skip2c > 0:\n",
    "            skip2c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            ret += i\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14780019-9aa6-4c52-a49a-c526220df1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aplicamos la función \"a\" a cada texto del corpus\n",
    "\n",
    "corpus = [a(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce0b97ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n♪ My face is the front of shop ♪\\n♪ My face is the real shop front ♪\\n♪ My shop is the face I front ♪\\n♪ I'm real when I shop my face ♪\\n♪ Artificial bloom ♪\\n♪ Hydroponic skin ♪\\n♪ Chemical release ♪\\n♪ Synthesize the real ♪\\n - Hey guys, it's Natalie,\\nwel\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ae884b-c67b-4443-b710-6b0f085cb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dividimos por salto de linea cada texto\n",
    "\n",
    "corpus = [c.replace('♪','').split('\\n') for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c624d8-96bd-449d-8378-ec716a91a987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' My face is the front of shop ',\n",
       " ' My face is the real shop front ',\n",
       " ' My shop is the face I front ',\n",
       " \" I'm real when I shop my face \"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac550f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eliminamos strings vacíos (cada texto)\n",
    "\n",
    "corpus = [[sentence.strip() for sentence in c if len(sentence)>0] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae6004d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## volvemos a juntar las oraciones :)\n",
    "\n",
    "corpus = [' '.join(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d62326-be95-4b3d-817f-a1bfbe6bc0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My face is the front of shop My face is the real shop front My shop is the face I front I'm real when I shop my face Artificial bloom Hydroponic skin Chemical release Synthesize the real - Hey guys, it's Natalie, welcome back to my channel. Today I'm\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34f16613-b9c2-4ec3-9a90-b9097145f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dividimos en oraciones cada texto\n",
    "\n",
    "sentence_list = []\n",
    "\n",
    "for texto in corpus:\n",
    "    sentences = sent_tokenize(texto)\n",
    "    sentence_list+=[sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "964d84c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ba60933-2a54-4a8d-9400-618c95b1cd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My face is the front of shop My face is the real shop front My shop is the face I front I'm real when I shop my face Artificial bloom Hydroponic skin Chemical release Synthesize the real - Hey guys, it's Natalie, welcome back to my channel.\",\n",
       " \"Today I'm gonna do a makeup tutorial, as always, but first, story time, story time, story time!\",\n",
       " \"I just wanna be upfront with you guys and let you know that I've had some facial surgery.\",\n",
       " \"I'm always gonna be honest with you guys about this kind of thing because you mean so much to me.\",\n",
       " \"Like you've been here with me since the beginning, and you've seen my story, my whole entire journey, this journey I've been on as a transgender woman.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c86e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtramos por textos con al menos 2 oraciones\n",
    "\n",
    "sentence_list = [s for s in sentence_list if len(s)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c12d5088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cab91a6-580d-477e-9af2-f5a533a787f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numero de oraciones por texto\n",
    "\n",
    "numero_oraciones = sum([len(s) for s in sentence_list])\n",
    "promedio_oraciones_por_texto = np.mean([len(s) for s in sentence_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fe5fdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7355, 319.7826086956522)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numero_oraciones, promedio_oraciones_por_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b513cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokens y types\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for texto in sentence_list:\n",
    "    for sentence in texto:\n",
    "        tokens+=sentence.split(' ')\n",
    "\n",
    "tokens = [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee0ee92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142029, 17593)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## número de tokens y types en el corpus\n",
    "\n",
    "len(tokens),len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "686584ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[322, 1041, 915, 73, 170, 128, 259, 264, 170, 333, 272, 367, 369, 424, 310, 425, 222, 317, 173, 318, 122, 201, 160]\n"
     ]
    }
   ],
   "source": [
    "# número de oraciones por texto\n",
    "num_sen = [len(s) for s in sentence_list]\n",
    "print(num_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85bc9c",
   "metadata": {},
   "source": [
    "## análisis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f2d03",
   "metadata": {},
   "source": [
    "### 1. Nominalizaciones\n",
    "El trabajo aquí es con listas de oraciones. Extraemos los sustantivos :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8870d8c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\pauba\\anaconda3\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (8.0.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "248d1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "oraciones_nlp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4563f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## identificamos el lema y el pos\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for sentence_text in sentence_list:\n",
    "    if len(sentence_text)>1:\n",
    "        sent = []\n",
    "        for sentence in sentence_text:\n",
    "            doc = nlp(sentence)\n",
    "            sent += [(token.lemma_.lower(),token.pos_) for token in doc]\n",
    "        oraciones_nlp += [sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "131ef0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oraciones_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1c211ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_tokens_types(oraciones):\n",
    "    number=[pair[0] for pair in oraciones]\n",
    "    return len(number),len(set(number)),len(set(number))/len(number)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31835204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6448, 1301, 20.176799007444167)\n",
      "(20874, 2303, 11.032863849765258)\n",
      "(16843, 2277, 13.518969304755684)\n",
      "(2089, 425, 20.34466251795117)\n",
      "(2998, 529, 17.6450967311541)\n",
      "(2792, 576, 20.630372492836678)\n",
      "(8217, 1496, 18.20615796519411)\n",
      "(7271, 1251, 17.2053362673635)\n",
      "(3523, 660, 18.734033494181094)\n",
      "(6488, 1253, 19.31257706535142)\n",
      "(5241, 1013, 19.328372448006107)\n",
      "(8595, 1507, 17.53344968004654)\n",
      "(8649, 1441, 16.660885651520406)\n",
      "(8296, 1256, 15.139826422372227)\n",
      "(7519, 1532, 20.37504987365341)\n",
      "(10654, 1488, 13.966585320067582)\n",
      "(6639, 1086, 16.357885223678263)\n",
      "(9705, 1423, 14.662545079855743)\n",
      "(3206, 796, 24.828446662507798)\n",
      "(8006, 1033, 12.902822882837873)\n",
      "(4441, 867, 19.52263003827967)\n",
      "(4947, 926, 18.718415201132)\n",
      "(3566, 837, 23.471676948962422)\n"
     ]
    }
   ],
   "source": [
    "## número de tokens y types para los 23 textos \n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    print(number_tokens_types(oraciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41bec5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_nouns(oraciones):\n",
    "    number = [pair for pair in oraciones if pair[1]=='NOUN']\n",
    "    return len(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0661b24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043\n",
      "2891\n",
      "2848\n",
      "312\n",
      "382\n",
      "382\n",
      "1292\n",
      "1095\n",
      "438\n",
      "968\n",
      "804\n",
      "1296\n",
      "1389\n",
      "1486\n",
      "1526\n",
      "1729\n",
      "1077\n",
      "1594\n",
      "504\n",
      "953\n",
      "618\n",
      "579\n",
      "474\n"
     ]
    }
   ],
   "source": [
    "## número de nouns tokens para los 23 textos \n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    print(number_nouns(oraciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2bdafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## la pregunta es: ¿De este número de nouns cuántos son nominalizaciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3894534",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reglas de nominalization\n",
    "\n",
    "## reglas de nominalization\n",
    "## puse las palabras lematizadas, en ese caso, solo es necesario poner las palabras en singular\n",
    "\n",
    "no_nom = ['thing', 'things', 'somethings', 'something', 'anything', 'everything', 'nothing','original', 'special', 'normal', 'version', 'tutorial', 'moment', 'comment', 'criminal', 'morning', 'tradition', 'notification','question', 'element', 'quality']\n",
    "terminacion = ['ment','ments','tions', 'tion','sions', 'sion', 'ibility','ibilities', 'ity','ities', 'ness','nesses', 'al','als','ings', 'ing'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87126c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominalization(oraciones):\n",
    "    nom = []\n",
    "    for pair in oraciones:\n",
    "        if pair[0] not in no_nom:\n",
    "            if pair[1]=='NOUN':\n",
    "                for END in terminacion:\n",
    "                    if pair[0].endswith(END):\n",
    "                        nom+=[pair[0]]\n",
    "    return nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "000c1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_list = []\n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    nom_list += [nominalization(oraciones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32401ff0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beginning',\n",
       " 'feminization',\n",
       " 'feminization',\n",
       " 'contouring',\n",
       " 'incision',\n",
       " 'fragment',\n",
       " 'incision',\n",
       " 'reconstruction',\n",
       " 'incision',\n",
       " 'exhibiting',\n",
       " 'anticipation',\n",
       " 'encouragement',\n",
       " 'jackal',\n",
       " 'vanity',\n",
       " 'feminization',\n",
       " 'reassignment',\n",
       " 'identity',\n",
       " 'argument',\n",
       " 'reality',\n",
       " 'loathing',\n",
       " 'ideal',\n",
       " 'transition',\n",
       " 'removal',\n",
       " 'thinking',\n",
       " 'realness',\n",
       " 'painting',\n",
       " 'femininity',\n",
       " 'aspiration',\n",
       " 'intensity',\n",
       " 'artificial',\n",
       " 'skinmaxing',\n",
       " 'business',\n",
       " 'discussion',\n",
       " 'masculinization',\n",
       " 'looksmaxing',\n",
       " 'deal',\n",
       " 'arousal',\n",
       " 'evolution',\n",
       " 'contouring',\n",
       " 'ingenuity',\n",
       " 'individuality',\n",
       " 'femininity',\n",
       " 'exploitation',\n",
       " 'bartering',\n",
       " 'judgment',\n",
       " 'exaggeration',\n",
       " 'delusion',\n",
       " 'embellishment',\n",
       " 'darkness',\n",
       " 'doctoring',\n",
       " 'invention',\n",
       " 'darkness',\n",
       " 'aging',\n",
       " 'ritual',\n",
       " 'cleansing',\n",
       " 'corporation',\n",
       " 'kindness',\n",
       " 'saving',\n",
       " 'collection',\n",
       " 'facial',\n",
       " 'futility',\n",
       " 'solution',\n",
       " 'spiral',\n",
       " 'obsession',\n",
       " 'advertising',\n",
       " 'relation',\n",
       " 'awareness',\n",
       " 'contemplation',\n",
       " 'revolution',\n",
       " 'revolution',\n",
       " 'critiquing',\n",
       " 'revolution',\n",
       " 'revolution',\n",
       " 'futility',\n",
       " 'representation',\n",
       " 'celebrity',\n",
       " 'spiral',\n",
       " 'ideal',\n",
       " 'originality',\n",
       " 'appreciation',\n",
       " 'superiority',\n",
       " 'solution',\n",
       " 'audacity',\n",
       " 'kindness']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nom_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9b57f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nom_list = []\n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    num_nom_list += [len(nominalization(oraciones)), len(nominalization(oraciones))/number_nouns(oraciones)*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c706d5a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84,\n",
       " 8.053691275167784,\n",
       " 377,\n",
       " 13.04047042545832,\n",
       " 338,\n",
       " 11.867977528089888,\n",
       " 38,\n",
       " 12.179487179487179,\n",
       " 34,\n",
       " 8.900523560209423,\n",
       " 26,\n",
       " 6.806282722513089,\n",
       " 147,\n",
       " 11.377708978328172,\n",
       " 81,\n",
       " 7.397260273972603,\n",
       " 41,\n",
       " 9.360730593607306,\n",
       " 115,\n",
       " 11.8801652892562,\n",
       " 78,\n",
       " 9.701492537313433,\n",
       " 135,\n",
       " 10.416666666666668,\n",
       " 136,\n",
       " 9.791216702663787,\n",
       " 234,\n",
       " 15.746971736204577,\n",
       " 253,\n",
       " 16.579292267365663,\n",
       " 164,\n",
       " 9.485251590514748,\n",
       " 95,\n",
       " 8.82079851439183,\n",
       " 125,\n",
       " 7.841907151819323,\n",
       " 62,\n",
       " 12.3015873015873,\n",
       " 104,\n",
       " 10.912906610703043,\n",
       " 83,\n",
       " 13.430420711974108,\n",
       " 75,\n",
       " 12.953367875647666,\n",
       " 43,\n",
       " 9.071729957805907]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## porcentaje de nominalizaciones con respecto al total de tokens sustantivos para cada texto\n",
    "\n",
    "num_nom_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e0540-21b6-4dd0-b89a-6dc6f4e44440",
   "metadata": {},
   "source": [
    "## 2. Academic World List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39a37341-2f7c-443e-af51-ddb1df93dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abriendo la lista de palabras academicas\n",
    "with open('AcademicWordList.txt') as f:\n",
    "    AWL=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d44543b-6da2-4619-b912-34c81138a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividir por salto de linea\n",
    "AWL= AWL.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e10d07b4-34e8-4325-997d-bef3e20a069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##eliminar espacios en blanco\n",
    "AWL = [palabra.split(' ') for palabra in AWL if len(palabra)>0]\n",
    "AWL = [item for sublist in AWL for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ffcef50-7f60-4c4f-a094-b63321bcda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWL.remove('-')\n",
    "AWL.remove('comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99c5dda9-db65-4030-ad1a-0d44525e63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = list(zip(*oraciones_nlp[0]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcc0dcd2-0dbd-4776-957b-cec6dc04023d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my', 'face', 'be', 'the', 'front', 'of', 'shop', 'my', 'face', 'be')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b31d8f3-d68f-4288-9788-56e877038e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142029"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd98ad-174c-4b9e-9e23-35ce5d7ed1c8",
   "metadata": {},
   "source": [
    "# PROBLEMAS AQUÍ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac27ab8b-196b-4433-a840-d2a75441591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def academic(tokens):\n",
    "    aca_list=[]\n",
    "    for word in tokens:\n",
    "        if word in AWL:\n",
    "            aca_list+=[word]\n",
    "    return aca_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "629534ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('my', 'PRON'),\n",
       " ('face', 'NOUN'),\n",
       " ('be', 'AUX'),\n",
       " ('the', 'DET'),\n",
       " ('front', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('shop', 'NOUN'),\n",
       " ('my', 'PRON'),\n",
       " ('face', 'NOUN'),\n",
       " ('be', 'VERB')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_nlp[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e2718b1-9efe-42bc-9ad6-7d8ff9c463ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "aca_words_percentage = []\n",
    "\n",
    "aca_words_per_text = []\n",
    "\n",
    "for oracion in oraciones_nlp:\n",
    "        texto = list(zip(*oracion))[0]\n",
    "        aca_words = academic(texto)\n",
    "        aca_words_per_text+=[aca_words]\n",
    "        aca_words_percentage += [len(aca_words)/len(texto)*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3342c095-142a-4ab3-a371-139ee3b77b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.271712158808933,\n",
       " 1.446775893455974,\n",
       " 1.733657899424093,\n",
       " 1.9626615605552895,\n",
       " 0.733822548365577,\n",
       " 0.6446991404011462,\n",
       " 1.2413289521723256,\n",
       " 1.2515472424700866,\n",
       " 2.8952597218279874,\n",
       " 1.495067817509248,\n",
       " 2.0415951154359857,\n",
       " 1.6055846422338569,\n",
       " 1.5839981500751534,\n",
       " 3.483606557377049,\n",
       " 3.1653145365075144,\n",
       " 2.543645579125211,\n",
       " 2.123813827383642,\n",
       " 2.215352910870685,\n",
       " 2.6512788521522146,\n",
       " 1.1241568823382464,\n",
       " 1.3960819635217292,\n",
       " 1.4958560743885183,\n",
       " 1.261918115535614]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aca_words_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e3beb-c8d5-4e20-8c17-e5c53ca7da44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
