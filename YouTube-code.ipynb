{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f493dc2",
   "metadata": {},
   "source": [
    "# Corpus YouTube"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30461b31",
   "metadata": {},
   "source": [
    "## 1. Lectura de datos!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc98573f-6644-4c5a-9a08-66dbbe12e0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## abrimos los archivos\n",
    "\n",
    "import glob\n",
    "\n",
    "lista_files = glob.glob('YTessays/*.txt')\n",
    "corpus = []\n",
    "\n",
    "for file in lista_files:\n",
    "    with open(file, 'r', encoding='utf8') as f:\n",
    "        corpus+=[f.read()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15522d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## número de textos\n",
    "\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f04e0950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(\"Faceshopping\" by Sophie)\\n♪ My face is the front of shop ♪\\n♪ My face is the real shop front ♪\\n♪ My shop is the face I front ♪\\n♪ I\\'m real when I shop my face ♪\\n♪ Artificial bloom ♪\\n♪ Hydroponic skin ♪\\n♪ Chemical release ♪\\n♪ Synthesize the real ♪\\n(upb'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea81bbb9",
   "metadata": {},
   "source": [
    "## 2. Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9dd9bf-b5ef-4505-8c67-ad3b187aea16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## librerias\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import string\n",
    "from nltk import sent_tokenize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e2fce56-63cb-495b-aaf8-fa7feae6b852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pauba\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20361ab2-6bda-46f4-b9bb-561d725c68e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## función para remover () y []\n",
    "\n",
    "def a(test_str):\n",
    "    ret = ''\n",
    "    skip1c = 0\n",
    "    skip2c = 0\n",
    "    for i in test_str:\n",
    "        if i == '[':\n",
    "            skip1c += 1\n",
    "        elif i == '(':\n",
    "            skip2c += 1\n",
    "        elif i == ']' and skip1c > 0:\n",
    "            skip1c -= 1\n",
    "        elif i == ')'and skip2c > 0:\n",
    "            skip2c -= 1\n",
    "        elif skip1c == 0 and skip2c == 0:\n",
    "            ret += i\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14780019-9aa6-4c52-a49a-c526220df1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aplicamos la función \"a\" a cada texto del corpus\n",
    "\n",
    "corpus = [a(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce0b97ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n♪ My face is the front of shop ♪\\n♪ My face is the real shop front ♪\\n♪ My shop is the face I front ♪\\n♪ I'm real when I shop my face ♪\\n♪ Artificial bloom ♪\\n♪ Hydroponic skin ♪\\n♪ Chemical release ♪\\n♪ Synthesize the real ♪\\n - Hey guys, it's Natalie,\\nwel\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6ae884b-c67b-4443-b710-6b0f085cb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dividimos por salto de linea cada texto\n",
    "\n",
    "corpus = [c.replace('♪','').split('\\n') for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34c624d8-96bd-449d-8378-ec716a91a987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " ' My face is the front of shop ',\n",
       " ' My face is the real shop front ',\n",
       " ' My shop is the face I front ',\n",
       " \" I'm real when I shop my face \"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac550f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## eliminamos strings vacíos (cada texto)\n",
    "\n",
    "corpus = [[sentence.strip() for sentence in c if len(sentence)>0] for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae6004d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## volvemos a juntar las oraciones\n",
    "\n",
    "corpus = [' '.join(c) for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d62326-be95-4b3d-817f-a1bfbe6bc0ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My face is the front of shop My face is the real shop front My shop is the face I front I'm real when I shop my face Artificial bloom Hydroponic skin Chemical release Synthesize the real - Hey guys, it's Natalie, welcome back to my channel. Today I'm\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34f16613-b9c2-4ec3-9a90-b9097145f6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dividimos en oraciones cada texto\n",
    "\n",
    "sentence_list = []\n",
    "\n",
    "for texto in corpus:\n",
    "    sentences = sent_tokenize(texto)\n",
    "    sentence_list+=[sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "964d84c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ba60933-2a54-4a8d-9400-618c95b1cd43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My face is the front of shop My face is the real shop front My shop is the face I front I'm real when I shop my face Artificial bloom Hydroponic skin Chemical release Synthesize the real - Hey guys, it's Natalie, welcome back to my channel.\",\n",
       " \"Today I'm gonna do a makeup tutorial, as always, but first, story time, story time, story time!\",\n",
       " \"I just wanna be upfront with you guys and let you know that I've had some facial surgery.\",\n",
       " \"I'm always gonna be honest with you guys about this kind of thing because you mean so much to me.\",\n",
       " \"Like you've been here with me since the beginning, and you've seen my story, my whole entire journey, this journey I've been on as a transgender woman.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_list[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c86e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## filtramos por textos con al menos 2 oraciones\n",
    "\n",
    "sentence_list = [s for s in sentence_list if len(s)>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c12d5088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cab91a6-580d-477e-9af2-f5a533a787f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numero de oraciones por texto\n",
    "\n",
    "numero_oraciones = sum([len(s) for s in sentence_list])\n",
    "promedio_oraciones_por_texto = np.mean([len(s) for s in sentence_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fe5fdfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7355, 319.7826086956522)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numero_oraciones, promedio_oraciones_por_texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b513cdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tokens y types\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for texto in sentence_list:\n",
    "    for sentence in texto:\n",
    "        tokens+=sentence.split(' ')\n",
    "\n",
    "tokens = [w.lower() for w in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee0ee92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(142029, 17593)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## número de tokens y types en el corpus\n",
    "\n",
    "len(tokens),len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "686584ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[322, 1041, 915, 73, 170, 128, 259, 264, 170, 333, 272, 367, 369, 424, 310, 425, 222, 317, 173, 318, 122, 201, 160]\n"
     ]
    }
   ],
   "source": [
    "# número de oraciones por texto\n",
    "num_sen = [len(s) for s in sentence_list]\n",
    "print(num_sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85bc9c",
   "metadata": {},
   "source": [
    "## análisis!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22f2d03",
   "metadata": {},
   "source": [
    "### 1. Nominalizaciones\n",
    "El trabajo aquí es con listas de oraciones. Extraemos los sustantivos :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8870d8c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\pauba\\anaconda3\\lib\\site-packages (3.0.6)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (1.7.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (1.20.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (52.0.0.post20210125)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (0.3.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (8.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "248d1911",
   "metadata": {},
   "outputs": [],
   "source": [
    "oraciones_nlp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4563f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "## identificamos el lema y el pos\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "for sentence_text in sentence_list:\n",
    "    if len(sentence_text)>1:\n",
    "        sent = []\n",
    "        for sentence in sentence_text:\n",
    "            doc = nlp(sentence)\n",
    "            sent += [(token.lemma_.lower(),token.pos_) for token in doc]\n",
    "        oraciones_nlp += [sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "131ef0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oraciones_nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1c211ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_tokens_types(oraciones):\n",
    "    number=[pair[0] for pair in oraciones]\n",
    "    return len(number),len(set(number)),len(set(number))/len(number)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31835204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6448, 1301, 20.176799007444167)\n",
      "(20874, 2303, 11.032863849765258)\n",
      "(16843, 2277, 13.518969304755684)\n",
      "(2089, 425, 20.34466251795117)\n",
      "(2998, 529, 17.6450967311541)\n",
      "(2792, 576, 20.630372492836678)\n",
      "(8217, 1496, 18.20615796519411)\n",
      "(7271, 1251, 17.2053362673635)\n",
      "(3523, 660, 18.734033494181094)\n",
      "(6488, 1253, 19.31257706535142)\n",
      "(5241, 1013, 19.328372448006107)\n",
      "(8595, 1507, 17.53344968004654)\n",
      "(8649, 1441, 16.660885651520406)\n",
      "(8296, 1256, 15.139826422372227)\n",
      "(7519, 1532, 20.37504987365341)\n",
      "(10654, 1488, 13.966585320067582)\n",
      "(6639, 1086, 16.357885223678263)\n",
      "(9705, 1423, 14.662545079855743)\n",
      "(3206, 796, 24.828446662507798)\n",
      "(8006, 1033, 12.902822882837873)\n",
      "(4441, 867, 19.52263003827967)\n",
      "(4947, 926, 18.718415201132)\n",
      "(3566, 837, 23.471676948962422)\n"
     ]
    }
   ],
   "source": [
    "## número de tokens y types para los 23 textos \n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    print(number_tokens_types(oraciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41bec5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_nouns(oraciones):\n",
    "    number = [pair for pair in oraciones if pair[1]=='NOUN']\n",
    "    return len(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0661b24",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1043\n",
      "2891\n",
      "2848\n",
      "312\n",
      "382\n",
      "382\n",
      "1292\n",
      "1095\n",
      "438\n",
      "968\n",
      "804\n",
      "1296\n",
      "1389\n",
      "1486\n",
      "1526\n",
      "1729\n",
      "1077\n",
      "1594\n",
      "504\n",
      "953\n",
      "618\n",
      "579\n",
      "474\n"
     ]
    }
   ],
   "source": [
    "## número de nouns tokens para los 23 textos \n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    print(number_nouns(oraciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2bdafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## la pregunta es: ¿De este número de nouns cuántos son nominalizaciones?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e3894534",
   "metadata": {},
   "outputs": [],
   "source": [
    "## reglas de nominalization\n",
    "\n",
    "## reglas de nominalization\n",
    "\n",
    "no_nom = ['thing', 'things', 'somethings', 'something', 'anything', 'everything', 'nothing','original', 'special', 'normal', 'version', 'tutorial', 'moment', 'comment', 'criminal', 'morning', 'tradition', 'notification','question', 'element', 'quality']\n",
    "terminacion = ['ment','ments','tions', 'tion','sions', 'sion', 'ibility','ibilities', 'ity','ities', 'ness','nesses', 'al','als','ings', 'ing'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87126c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nominalization(oraciones):\n",
    "    nom = []\n",
    "    for pair in oraciones:\n",
    "        if pair[0] not in no_nom:\n",
    "            if pair[1]=='NOUN':\n",
    "                for END in terminacion:\n",
    "                    if pair[0].endswith(END):\n",
    "                        nom+=[pair[0]]\n",
    "    return nom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "000c1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_list = []\n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    nom_list += [nominalization(oraciones)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32401ff0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beginning',\n",
       " 'feminization',\n",
       " 'feminization',\n",
       " 'contouring',\n",
       " 'incision',\n",
       " 'fragment',\n",
       " 'incision',\n",
       " 'reconstruction',\n",
       " 'incision',\n",
       " 'exhibiting',\n",
       " 'anticipation',\n",
       " 'encouragement',\n",
       " 'jackal',\n",
       " 'vanity',\n",
       " 'feminization',\n",
       " 'reassignment',\n",
       " 'identity',\n",
       " 'argument',\n",
       " 'reality',\n",
       " 'loathing',\n",
       " 'ideal',\n",
       " 'transition',\n",
       " 'removal',\n",
       " 'thinking',\n",
       " 'realness',\n",
       " 'painting',\n",
       " 'femininity',\n",
       " 'aspiration',\n",
       " 'intensity',\n",
       " 'artificial',\n",
       " 'skinmaxing',\n",
       " 'business',\n",
       " 'discussion',\n",
       " 'masculinization',\n",
       " 'looksmaxing',\n",
       " 'deal',\n",
       " 'arousal',\n",
       " 'evolution',\n",
       " 'contouring',\n",
       " 'ingenuity',\n",
       " 'individuality',\n",
       " 'femininity',\n",
       " 'exploitation',\n",
       " 'bartering',\n",
       " 'judgment',\n",
       " 'exaggeration',\n",
       " 'delusion',\n",
       " 'embellishment',\n",
       " 'darkness',\n",
       " 'doctoring',\n",
       " 'invention',\n",
       " 'darkness',\n",
       " 'aging',\n",
       " 'ritual',\n",
       " 'cleansing',\n",
       " 'corporation',\n",
       " 'kindness',\n",
       " 'saving',\n",
       " 'collection',\n",
       " 'facial',\n",
       " 'futility',\n",
       " 'solution',\n",
       " 'spiral',\n",
       " 'obsession',\n",
       " 'advertising',\n",
       " 'relation',\n",
       " 'awareness',\n",
       " 'contemplation',\n",
       " 'revolution',\n",
       " 'revolution',\n",
       " 'critiquing',\n",
       " 'revolution',\n",
       " 'revolution',\n",
       " 'futility',\n",
       " 'representation',\n",
       " 'celebrity',\n",
       " 'spiral',\n",
       " 'ideal',\n",
       " 'originality',\n",
       " 'appreciation',\n",
       " 'superiority',\n",
       " 'solution',\n",
       " 'audacity',\n",
       " 'kindness']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nom_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9b57f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nom_list = []\n",
    "\n",
    "for oraciones in oraciones_nlp:\n",
    "    num_nom_list += [len(nominalization(oraciones)), len(nominalization(oraciones))/number_nouns(oraciones)*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c706d5a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[84,\n",
       " 8.053691275167784,\n",
       " 377,\n",
       " 13.04047042545832,\n",
       " 338,\n",
       " 11.867977528089888,\n",
       " 38,\n",
       " 12.179487179487179,\n",
       " 34,\n",
       " 8.900523560209423,\n",
       " 26,\n",
       " 6.806282722513089,\n",
       " 147,\n",
       " 11.377708978328172,\n",
       " 81,\n",
       " 7.397260273972603,\n",
       " 41,\n",
       " 9.360730593607306,\n",
       " 115,\n",
       " 11.8801652892562,\n",
       " 78,\n",
       " 9.701492537313433,\n",
       " 135,\n",
       " 10.416666666666668,\n",
       " 136,\n",
       " 9.791216702663787,\n",
       " 234,\n",
       " 15.746971736204577,\n",
       " 253,\n",
       " 16.579292267365663,\n",
       " 164,\n",
       " 9.485251590514748,\n",
       " 95,\n",
       " 8.82079851439183,\n",
       " 125,\n",
       " 7.841907151819323,\n",
       " 62,\n",
       " 12.3015873015873,\n",
       " 104,\n",
       " 10.912906610703043,\n",
       " 83,\n",
       " 13.430420711974108,\n",
       " 75,\n",
       " 12.953367875647666,\n",
       " 43,\n",
       " 9.071729957805907]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## porcentaje de nominalizaciones con respecto al total de tokens sustantivos para cada texto\n",
    "\n",
    "num_nom_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281e0540-21b6-4dd0-b89a-6dc6f4e44440",
   "metadata": {},
   "source": [
    "## 2. Academic World List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39a37341-2f7c-443e-af51-ddb1df93dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#abriendo la lista de palabras academicas\n",
    "with open('AcademicWordList.txt') as f:\n",
    "    AWL=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d44543b-6da2-4619-b912-34c81138a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividir por salto de linea\n",
    "AWL= AWL.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e10d07b4-34e8-4325-997d-bef3e20a069c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##eliminar espacios en blanco\n",
    "AWL = [palabra.split(' ') for palabra in AWL if len(palabra)>0]\n",
    "AWL = [item for sublist in AWL for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ffcef50-7f60-4c4f-a094-b63321bcda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "AWL.remove('-')\n",
    "AWL.remove('comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "99c5dda9-db65-4030-ad1a-0d44525e63ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = list(zip(*oraciones_nlp[0]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcc0dcd2-0dbd-4776-957b-cec6dc04023d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my', 'face', 'be', 'the', 'front', 'of', 'shop', 'my', 'face', 'be')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texto[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b31d8f3-d68f-4288-9788-56e877038e6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142029"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ac27ab8b-196b-4433-a840-d2a75441591e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# función separadora de academic words\n",
    "def academic(tokens):\n",
    "    aca_list=[]\n",
    "    for word in tokens:\n",
    "        if word in AWL:\n",
    "            aca_list+=[word]\n",
    "    return aca_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "629534ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('my', 'PRON'),\n",
       " ('face', 'NOUN'),\n",
       " ('be', 'AUX'),\n",
       " ('the', 'DET'),\n",
       " ('front', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('shop', 'NOUN'),\n",
       " ('my', 'PRON'),\n",
       " ('face', 'NOUN'),\n",
       " ('be', 'VERB')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oraciones_nlp[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e2718b1-9efe-42bc-9ad6-7d8ff9c463ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "aca_words_percentage = []\n",
    "\n",
    "aca_words_per_text = []\n",
    "\n",
    "for oracion in oraciones_nlp:\n",
    "        texto = list(zip(*oracion))[0]\n",
    "        aca_words = academic(texto)\n",
    "        aca_words_per_text+=[aca_words]\n",
    "        aca_words_percentage += [len(aca_words)/len(texto)*100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3342c095-142a-4ab3-a371-139ee3b77b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.271712158808933,\n",
       " 1.446775893455974,\n",
       " 1.733657899424093,\n",
       " 1.9626615605552895,\n",
       " 0.733822548365577,\n",
       " 0.6446991404011462,\n",
       " 1.2413289521723256,\n",
       " 1.2515472424700866,\n",
       " 2.8952597218279874,\n",
       " 1.495067817509248,\n",
       " 2.0415951154359857,\n",
       " 1.6055846422338569,\n",
       " 1.5839981500751534,\n",
       " 3.483606557377049,\n",
       " 3.1653145365075144,\n",
       " 2.543645579125211,\n",
       " 2.123813827383642,\n",
       " 2.215352910870685,\n",
       " 2.6512788521522146,\n",
       " 1.1241568823382464,\n",
       " 1.3960819635217292,\n",
       " 1.4958560743885183,\n",
       " 1.261918115535614]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aca_words_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba94858-c6b8-4203-950d-3326a7b3de14",
   "metadata": {},
   "source": [
    "## READABILITY\n",
    "Aquí deberían darme números entre 0 y 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f472ea75-ae95-4f46-9247-532b7c3cdf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py-readability-metrics in c:\\users\\pauba\\anaconda3\\lib\\site-packages (1.4.5)\n",
      "Requirement already satisfied: nltk in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from py-readability-metrics) (3.6.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from nltk->py-readability-metrics) (1.0.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from nltk->py-readability-metrics) (4.59.0)\n",
      "Requirement already satisfied: regex in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from nltk->py-readability-metrics) (2021.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from nltk->py-readability-metrics) (7.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install py-readability-metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "750b5986-d049-4ee0-aab9-0b887c0ad344",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-740d187ee9af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moracion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mReadability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mfk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflesch_kincaid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\readability\\readability.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_analyzer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAnalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_statistics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_analyzer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mari\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\readability\\text\\analyzer.py\u001b[0m in \u001b[0;36manalyze\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dale_chall_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_dale_chall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_spache_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_load_spache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sentences'\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# hack for smog\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mAnalyzerStatistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\readability\\text\\analyzer.py\u001b[0m in \u001b[0;36m_statistics\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0msyllable_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mpoly_syllable_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\readability\\text\\analyzer.py\u001b[0m in \u001b[0;36m_tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTweetTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_is_punctuation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \"\"\"\n\u001b[0;32m    285\u001b[0m         \u001b[1;31m# Fix HTML character entities:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_replace_html_entities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m         \u001b[1;31m# Remove username handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip_handles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tokenize\\casual.py\u001b[0m in \u001b[0;36m_replace_html_entities\u001b[1;34m(text, keep, remove_illegal, encoding)\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m\"\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mremove_illegal\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mENT_RE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_entity\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_str_to_unicode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or buffer"
     ]
    }
   ],
   "source": [
    "##Flesch Kincaid Grade Level\n",
    "\n",
    "from readability import Readability\n",
    "\n",
    "text = list(zip(*oracion))[0]\n",
    "\n",
    "r=Readability(text)\n",
    "fk=r.flesch_kincaid()\n",
    "\n",
    "print(fk.score)\n",
    "print(fk.grade_lever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd701890-6313-47af-aca2-90cd95650391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c5fe8d8-5860-466e-a29a-dc0b47565386",
   "metadata": {},
   "source": [
    "## Passive Voice\n",
    "Esto debería separarme oraciones que están en voz pasiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8400b20f-400a-4386-a963-a4ac2f62aaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz (13.7 MB)\n",
      "Requirement already satisfied: spacy<3.1.0,>=3.0.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.0.0) (3.0.6)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.5)\n",
      "Requirement already satisfied: pydantic<1.8.0,>=1.7.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.7.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (20.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.4 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (3.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.20.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.8.2)\n",
      "Requirement already satisfied: typer<0.4.0,>=0.3.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.3.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (0.6.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.3 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (8.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.59.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (5.2.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (4.0.0)\n",
      "Requirement already satisfied: click<7.2.0,>=7.1.1 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from typer<0.4.0,>=0.3.0->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\pauba\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.1.0,>=3.0.0->en-core-web-sm==3.0.0) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.0.0/en_core_web_sm-3.0.0.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fe3b45c5-28bb-4ec3-98e3-47fd96a0c5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a2991c6a-fc38-4007-8e9f-fd58e83ac4af",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'list'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-1df2de5a277a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# reading the list of test sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[0mdfs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'YTessays/*.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m     \u001b[1;31m# open URLs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[0;32m    559\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    369\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    370\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf\"Invalid file path or buffer object type: {type(filepath_or_buffer)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 371\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     return IOArgs(\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'list'>"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "# function to check the type of sentence\n",
    "def checkForSentType(inputSentence):   \n",
    "    # running the model on sentence\n",
    "    getDocFile = nlp(inputSentence)\n",
    "    \n",
    "    # getting the syntactic dependency \n",
    "    getAllTags = [token.dep_ for token in getDocFile]\n",
    "    \n",
    "    # checking for 'agent' tag\n",
    "    checkPassiveTest = any(['agent' in sublist for sublist in getAllTags])\n",
    "    \n",
    "    # checking for 'nsubjpass' tag\n",
    "    checkPassiveTestTwo = any(['nsubjpass' in sublist for sublist in getAllTags])\n",
    "    return checkPassiveTest or checkPassiveTestTwo\n",
    "\n",
    "# Spacy model imported\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# reading the list of test sentences\n",
    "dfs = pd.read_csv(glob.glob('YTessays/*.txt'))\n",
    "sentences = dfs.values.tolist()\n",
    "\n",
    "finalResult = []\n",
    "\n",
    "# checking each sentence for its type\n",
    "for sentence in sentences:\n",
    "    result = checkForSentType(str(sentence))\n",
    "    if(result):\n",
    "        finalResult.append('Passive Sentence')\n",
    "    else:\n",
    "        finalResult.append('Active Sentence')\n",
    "        \n",
    "# storing the result in a new file and converting to csv\n",
    "newDf = pd.DataFrame({'Sentences':sentences,'Answers':finalResult})\n",
    "\n",
    "newDf.to_csv('Sentence_Identified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70960301-4f63-4028-b26a-f71c373f16f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
